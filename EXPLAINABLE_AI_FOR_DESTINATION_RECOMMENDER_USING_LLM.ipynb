{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVG3YyxhmwKG"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9vxUfvTqmmWY",
        "outputId": "778df8ef-4700-436e-d1c8-fec4f4c2c6ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.47.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.48.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.47.1-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.47.1 watchdog-6.0.0\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for segment_anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K\n",
            "added 22 packages in 2s\n",
            "\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K"
          ]
        }
      ],
      "source": [
        "# Setup - Install Dependencies\n",
        "\n",
        "!pip install streamlit\n",
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q git+https://github.com/facebookresearch/segment-anything.git\n",
        "!pip install -q opencv-python matplotlib transformers ftfy regex faiss-cpu\n",
        "!pip install -q transformers faiss-cpu\n",
        "!pip install -q transformers accelerate\n",
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NFCKB2VUmmWZ"
      },
      "outputs": [],
      "source": [
        "# Download SAM Checkpoint (only once)\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth -O sam_vit_b.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_S-7_1UmmWa",
        "outputId": "f70f458f-b4aa-447c-a8f8-d7b0e871fb43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "from PIL import Image\n",
        "import time\n",
        "import os\n",
        "from io import BytesIO\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "from torchvision import transforms\n",
        "import requests\n",
        "import re\n",
        "import faiss\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from huggingface_hub import InferenceClient\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "from transformers import CLIPProcessor, CLIPModel, BlipProcessor, BlipForConditionalGeneration, AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def call_llm():\n",
        "    response = client.chat.completions.create(\n",
        "      model=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "      messages=[\n",
        "        {\"role\": \"user\", \"content\": llm_prompt}\n",
        "      ],\n",
        "      temperature=0.7,\n",
        "      max_tokens=200\n",
        "    )\n",
        "\n",
        "    result = response['choices'][0]['message']['content'].strip()\n",
        "    return result\n",
        "\n",
        "import random\n",
        "\n",
        "def build_prompt(tags, destination):\n",
        "\n",
        "  # Safe fallback for unknown location\n",
        "    location = destination['location']\n",
        "    if not location or location.lower() == 'unknown':\n",
        "        location_phrase = \"this stunning landscape\"\n",
        "        alt_location_sentence = \"This scenery is truly a hidden gem waiting to be explored.\"\n",
        "    else:\n",
        "        location_phrase = location\n",
        "        alt_location_sentence = f\"A similar travel destination could be '{location}'.\"\n",
        "\n",
        "    similar_phrases = [\n",
        "        f\"A similar travel destination could be \\\"{destination['location']}\\\".\",\n",
        "        f\"Another amazing place you might love is \\\"{destination['location']}\\\".\",\n",
        "        f\"A great place to consider is \\\"{destination['location']}\\\".\"\n",
        "    ]\n",
        "    chosen_phrase = random.choice(similar_phrases)\n",
        "\n",
        "    return (\n",
        "        f\"\"\"\n",
        "        The user wants a destination that matches these tags: {tags}.\n",
        "\n",
        "        The most visually similar destination is: {destination['location']}.\n",
        "        Destination description: {destination['description']}.\n",
        "\n",
        "        If the location is unknown, suggest a real or plausible location that matches the scenery based on the tags. If you can't, describe the scenery naturally without naming a place.\n",
        "\n",
        "        Write a short explanation in exactly 8 sentences:\n",
        "        1 Start with an sentence like : \"What a stunning place!\"\n",
        "        2 Then say: \"{chosen_phrase}\"\n",
        "        3 Then say: {alt_location_sentence}\n",
        "        4 In the next sentence, briefly introduce the place (where it is, what it is).\n",
        "        5 In the next sentence, highlight what makes it special.\n",
        "        6 In the next sentence, describe what makes it visually or culturally unique.\n",
        "        7 In the next sentence, say why it‚Äôs famous and why someone should visit.\n",
        "        8 End with a friendly invitation to visit ‚Äî keep it short and warm.\n",
        "        Make this one paragraph. Do not include numbers. Keep the whole explanation in 200 tokens. Do not exceed this.\n",
        "\n",
        "        IMPORTANT:\n",
        "        - Make sure the description strictly relates to the tags and factual information about the location.\n",
        "        - Avoid speculation or adding information not supported by the tags or description.\n",
        "        - Maintain clarity and avoid generic statements.\n",
        "        - Do not repeat phrases unnecessarily.\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "def build_feedback_prompt(tags, destination):\n",
        "\n",
        "    feedback_prompt = f\"\"\"\n",
        "    The user wants a destination that matches these tags: {tags}.\n",
        "\n",
        "    The most visually similar destination is: {destination['location']}.\n",
        "    Destination description: {destination['description']}.\n",
        "\n",
        "    If the location is unknown, suggest a real or plausible location that matches the scenery based on the tags. If you can't, describe the scenery naturally without naming a place.\n",
        "\n",
        "    Write a single, warm, descriptive message in **exactly 5 to 7 complete sentences**.\n",
        "    Start with a phrase like ‚ÄúOh, I see you want...‚Äù referencing the user‚Äôs interest.\n",
        "    Mention the place name naturally and speak directly to the reader (do not use ‚Äúwe‚Äù).\n",
        "    Describe what makes it special, what they can see or do there, and inspire them to visit.\n",
        "\n",
        "\n",
        "    IMPORTANT:\n",
        "        - Make sure the description strictly relates to the tags and factual information about the location.\n",
        "        - Avoid speculation or adding information not supported by the tags or description.\n",
        "        - Maintain clarity and avoid generic statements.\n",
        "        - Do not repeat phrases unnecessarily.\n",
        "        - Give one output suggestion only.\n",
        "        - Do not include instructions ‚Äî output only the final message.\n",
        "        - Keep the total length under 200 tokens.\n",
        "\n",
        "    Output only the final text.\n",
        "    \"\"\"\n",
        "    return feedback_prompt\n",
        "\n",
        "\n",
        "####################################### VisionAgent #######################################################\n",
        "\n",
        "# Load SAM model\n",
        "sam = sam_model_registry[\"vit_b\"](checkpoint=\"sam_vit_b.pth\")\n",
        "sam.to(device)\n",
        "predictor = SamPredictor(sam)\n",
        "\n",
        "# Load BLIP model\n",
        "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
        "\n",
        "# Load CLIP model\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Generate caption\n",
        "def generate_caption(image_pil):\n",
        "    inputs = blip_processor(image_pil, return_tensors=\"pt\").to(device)\n",
        "    out = blip_model.generate(**inputs,do_sample=True,\n",
        "        top_p=0.9,            # Nucleus sampling\n",
        "        temperature=0.8,       # Add variability\n",
        "        max_length=100,\n",
        "        repetition_penalty=1.2)\n",
        "    caption = blip_processor.decode(out[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "# Generate segment and caption\n",
        "def segment_and_caption_image(image):\n",
        "    # Convert to NumPy array for downstream use\n",
        "    img = image.convert(\"RGB\")\n",
        "    image_np = np.array(img)\n",
        "\n",
        "    # Convert to OpenCV format (BGR)\n",
        "    image_bgr = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\n",
        "    predictor.set_image(image_bgr)\n",
        "\n",
        "    # Input point in the center of the image\n",
        "    input_point = np.array([[image.width // 2, image.height // 2]])\n",
        "    input_label = np.array([1])  # 1 = positive label\n",
        "\n",
        "    # Generate segmentation masks\n",
        "    masks, scores, _ = predictor.predict(\n",
        "        point_coords=input_point,\n",
        "        point_labels=input_label,\n",
        "        multimask_output=True\n",
        "    )\n",
        "\n",
        "    caption = generate_caption(image)\n",
        "    return caption\n",
        "\n",
        "def extract_keywords_from_caption(caption_):\n",
        "    if not isinstance(caption, str) or not caption.strip():\n",
        "        return []\n",
        "\n",
        "    keywords = re.findall(r'\\b\\w+\\b', caption.lower())\n",
        "    stopwords = {\"a\", \"the\", \"at\", \"on\", \"in\", \"with\", \"and\", \"of\", \"to\", \"is\", \"by\", \"an\"}\n",
        "    return [kw for kw in keywords if kw not in stopwords]\n",
        "\n",
        "\n",
        "################################## DestinationAgent ###############################################\n",
        "\n",
        "# Load unsplash data\n",
        "# Get filenames\n",
        "index_file = \"/content/unsplash_clip.index\"\n",
        "csv_file = \"/content/unsplash_clip_metadata.csv\"\n",
        "\n",
        "# Load files\n",
        "faiss_index = faiss.read_index(index_file)\n",
        "metadata_df = pd.read_csv(csv_file)\n",
        "\n",
        "def get_best_location(row):\n",
        "    for col in [\n",
        "        \"photo_location_city\",\n",
        "        \"photo_location_country\",\n",
        "        \"ai_primary_landmark_name\"\n",
        "    ]:\n",
        "        val = row.get(col)\n",
        "        if isinstance(val, str) and val.strip():\n",
        "            return val.strip()\n",
        "    return \"Unknown\"\n",
        "\n",
        "metadata_df[\"resolved_location\"] = metadata_df.apply(get_best_location, axis=1)\n",
        "\n",
        "#Setup Embedding Function\n",
        "def get_clip_embedding(text):\n",
        "    inputs = clip_processor(text=[text], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        return clip_model.get_text_features(**inputs).cpu().numpy().astype(\"float32\")\n",
        "\n",
        "#Define Matching Function with BLIP Tags\n",
        "def match_from_blip_tags(blip_tags, top_k):\n",
        "    query = \", \".join(blip_tags)\n",
        "    query_emb = get_clip_embedding(query)\n",
        "\n",
        "    distances, indices = faiss_index.search(query_emb, top_k)\n",
        "\n",
        "    results = []\n",
        "    for i, idx in enumerate(indices[0]):\n",
        "        row = metadata_df.iloc[idx]\n",
        "        results.append({\n",
        "            \"rank\": i + 1,\n",
        "            \"photo_id\": row.get(\"photo_id\"),\n",
        "            \"image_url\": row.get(\"photo_image_url\"),\n",
        "            \"description\": row.get(\"ai_description\"),\n",
        "            \"location\": row.get(\"resolved_location\", \"Unknown\") if \"resolved_location\" in row else \"Unknown\",\n",
        "            \"distance\": float(distances[0][i])\n",
        "        })\n",
        "    return results\n",
        "\n",
        "\n",
        "#################################### Feedback Agent ##################################################\n",
        "# Initialize HuggingFace API client\n",
        "client = InferenceClient(\n",
        "    provider=\"featherless-ai\",\n",
        "    api_key='[YOUR_HUGGING_FACE_TOKEN]',\n",
        ")\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# Predefined map of adjectives to tags (90 entries)\n",
        "adjective_map = {\n",
        "    \"Beautiful\": \"scenic\", \"Scenic\": \"scenic\",\n",
        "    \"Cheap\": \"affordable\", \"Affordable\": \"affordable\",\n",
        "    \"Crowded\": \"overpopulated\", \"Overpopulated\": \"overpopulated\",\n",
        "    \"Expensive\": \"luxury\", \"Luxury\": \"luxury\",\n",
        "    \"Quiet\": \"peaceful\", \"Peaceful\": \"peaceful\",\n",
        "    \"Rustic\": \"rural\", \"Rural\": \"rural\",\n",
        "    \"Romantic\": \"romantic\", \"Romantic\": \"romantic\",\n",
        "    \"Clean\": \"hygienic\", \"Hygienic\": \"hygienic\",\n",
        "    \"Warm\": \"tropical\", \"Tropical\": \"tropical\",\n",
        "    \"Cold\": \"winter\", \"Winter\": \"winter\",\n",
        "    \"Busy\": \"urban\", \"Urban\": \"urban\",\n",
        "    \"Relaxing\": \"tranquil\", \"Tranquil\": \"tranquil\",\n",
        "    \"Charming\": \"quaint\", \"Quaint\": \"quaint\",\n",
        "    \"Historic\": \"cultural\", \"Cultural\": \"cultural\",\n",
        "    \"Adventurous\": \"adventure\", \"Adventure\": \"adventure\",\n",
        "    \"Safe\": \"secure\", \"Secure\": \"secure\",\n",
        "    \"Family-friendly\": \"family\", \"Family\": \"family\",\n",
        "    \"Friendly\": \"hospitable\", \"Hospitable\": \"hospitable\",\n",
        "    \"Lively\": \"vibrant\", \"Vibrant\": \"vibrant\",\n",
        "    \"Modern\": \"urban\", \"Urban\": \"urban\",\n",
        "    \"Tropical\": \"island\", \"Island\": \"island\",\n",
        "    \"Unique\": \"exotic\", \"Exotic\": \"exotic\",\n",
        "    \"Vibrant\": \"lively\", \"Lively\": \"lively\",\n",
        "    \"Isolated\": \"remote\", \"Remote\": \"remote\",\n",
        "    \"Spacious\": \"expansive\", \"Expansive\": \"expansive\",\n",
        "    \"Luxury\": \"high-end\", \"High-end\": \"high-end\",\n",
        "    \"Breathtaking\": \"scenic\", \"Scenic\": \"scenic\",\n",
        "    \"Peaceful\": \"calm\", \"Calm\": \"calm\",\n",
        "    \"Exotic\": \"adventurous\", \"Adventurous\": \"adventurous\",\n",
        "    \"Picturesque\": \"scenic\", \"Scenic\": \"scenic\",\n",
        "    \"Majestic\": \"grand\", \"Grand\": \"grand\",\n",
        "    \"Aesthetic\": \"artistic\", \"Artistic\": \"artistic\",\n",
        "    \"Serene\": \"calm\", \"Calm\": \"calm\",\n",
        "    \"Idyllic\": \"peaceful\", \"Peaceful\": \"peaceful\",\n",
        "    \"Dynamic\": \"vibrant\", \"Vibrant\": \"vibrant\",\n",
        "    \"Enchanting\": \"charming\", \"Charming\": \"charming\",\n",
        "    \"Mysterious\": \"exotic\", \"Exotic\": \"exotic\",\n",
        "    \"Thriving\": \"urban\", \"Urban\": \"urban\",\n",
        "    \"Chilly\": \"cold\", \"Cold\": \"cold\",\n",
        "    \"Diverse\": \"multicultural\", \"Multicultural\": \"multicultural\",\n",
        "    \"Wilderness\": \"nature\", \"Nature\": \"nature\",\n",
        "    \"Glistening\": \"shiny\", \"Shiny\": \"shiny\",\n",
        "    \"Untouched\": \"pristine\", \"Pristine\": \"pristine\",\n",
        "    \"Cozy\": \"comfortable\", \"Comfortable\": \"comfortable\",\n",
        "    \"Lush\": \"green\", \"Green\": \"green\",\n",
        "    \"Bright\": \"sunny\", \"Sunny\": \"sunny\",\n",
        "    \"Vast\": \"expansive\", \"Expansive\": \"expansive\",\n",
        "    \"Picturesque\": \"scenic\", \"Scenic\": \"scenic\",\n",
        "    \"Delightful\": \"charming\", \"Charming\": \"charming\",\n",
        "    \"Vibrant\": \"colorful\", \"Colorful\": \"colorful\",\n",
        "    \"Fascinating\": \"intriguing\", \"Intriguing\": \"intriguing\",\n",
        "    \"Sandy\": \"beach\", \"Beach\": \"beach\",\n",
        "    \"Luminous\": \"bright\", \"Bright\": \"bright\",\n",
        "    \"Breezy\": \"refreshing\", \"Refreshing\": \"refreshing\",\n",
        "    \"Clear\": \"crystal-clear\", \"Crystal-clear\": \"crystal-clear\"\n",
        "}\n",
        "\n",
        "def extract_adjectives(feedback_text):\n",
        "    \"\"\"\n",
        "    Use spaCy POS tagging to extract adjectives from the feedback text.\n",
        "    \"\"\"\n",
        "    doc = nlp(feedback_text)\n",
        "    adjectives = [token.text.capitalize() for token in doc if token.pos_ == \"ADJ\"]\n",
        "    return adjectives\n",
        "\n",
        "# Handle special phrases (e.g., \"not too crowded\")\n",
        "def process_special_phrases(feedback_text, original_tags):\n",
        "    special_phrases = {\n",
        "        \"not too crowded\": \"peaceful\",\n",
        "        \"not too expensive\": \"affordable\",\n",
        "        \"too crowded\": \"overpopulated\",\n",
        "        \"too expensive\": \"luxury\",\n",
        "        \"too hot\": \"tropical\",\n",
        "        \"not too hot\": \"tropical\",\n",
        "        \"not too cold\": \"tropical\",\n",
        "        \"too cold\": \"winter\",\n",
        "        \"not too far\": \"near\",\n",
        "        \"not too close\": \"remote\",\n",
        "        \"not too long\": \"short\",\n",
        "        \"not too short\": \"long\",\n",
        "        \"not too busy\": \"peaceful\",\n",
        "        \"too busy\": \"urban\",\n",
        "        \"not too loud\": \"quiet\",\n",
        "        \"too loud\": \"noisy\",\n",
        "        \"not too bright\": \"dim\",\n",
        "        \"too bright\": \"sunny\",\n",
        "        \"not too rainy\": \"dry\",\n",
        "        \"too rainy\": \"wet\"\n",
        "    }\n",
        "\n",
        "    for phrase, tag in special_phrases.items():\n",
        "        if phrase in feedback_text.lower():\n",
        "            if tag not in original_tags:\n",
        "                original_tags.append(tag)\n",
        "            feedback_text = feedback_text.lower().replace(phrase, '')\n",
        "\n",
        "    return feedback_text\n",
        "\n",
        "\n",
        "\n",
        "def fallback_to_llm(feedback_text: str, original_tags: list):\n",
        "    print(\"Fallback to LLM\")\n",
        "    # Fallback to LLM\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\"\n",
        "                Classify the following text as \"feedback\", \"query\", or \"assertive chat\".\n",
        "                If it's feedback or a query, provide the relevant tags based on the text and the original tags. Do not put any comment. Just tags please. For each relevant adjective that is not present in the original\n",
        "                text provide one tag in one word that best fits the adjective you don't have to provide its similar or synonymous tags. Be very diligent in providing the tags. Like not too hot can be cold or warm. The tags has\n",
        "                to be words. no characters please very mindful regarding this. The tags can also be noun like place name or sight name. You can keep the original tags if you think they are relevant.\n",
        "                Return only the tags, in a comma-separated list.\n",
        "                If it's assertive chat, return \"null\".\n",
        "\n",
        "                Example 1:\n",
        "                Input: \"Can you show me something cool?\"\n",
        "                Original tags: [\"nature\", \"mountain\"]\n",
        "                Output: \"affordable, scenic, cool\"\n",
        "\n",
        "                Example 2:\n",
        "                Input: \"What is the weather like in Switzerland?\"\n",
        "                Original tags: [\"travel\", \"weather\"]\n",
        "                Output: \"mountain, Switzerland, adventure\"\n",
        "\n",
        "                Example 3:\n",
        "                Input: \"I don't care about the price, just show me something unique.\"\n",
        "                Original tags: [\"nature\", \"adventure\"]\n",
        "                Output: \"unique, scenic, exotic\"\n",
        "\n",
        "                Example 4:\n",
        "                Input: \"Great\"\n",
        "                Original tags: [\"nature\", \"mountain\"]\n",
        "                Output: \"null\"\n",
        "\n",
        "                Example 5:\n",
        "                Input: \"Thank You\"\n",
        "                Original tags: [\"nature\", \"mountain\"]\n",
        "                Output: \"null\"\n",
        "\n",
        "\n",
        "                Input:\n",
        "                \"{feedback_text}\"\n",
        "                Original tags: {original_tags}\n",
        "                \"\"\"\n",
        "            }\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    result = response['choices'][0]['message']['content'].strip()\n",
        "\n",
        "    if result.lower() != \"null\":\n",
        "        tags = [tag.strip().strip('\"') for tag in result.split(',')]\n",
        "        return tags\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Main processing function\n",
        "def process_feedback(feedback_text: str, original_tags: list):\n",
        "    \"\"\"\n",
        "    Detect feedback/query, then extract relevant tags either from the map or fallback to LLM.\n",
        "    \"\"\"\n",
        "    feedback_text = process_special_phrases(feedback_text, original_tags)  # Handle special phrases\n",
        "\n",
        "    words = feedback_text.lower().split()\n",
        "\n",
        "    adjectives = extract_adjectives(feedback_text)\n",
        "\n",
        "    # Find tags from predefined map\n",
        "    # tags = [adjective_map.get(word.capitalize(), None) for word in words]\n",
        "    tags = [adjective_map.get(adj, None) for adj in adjectives]\n",
        "\n",
        "    tags = list(set(tag for tag in tags if tag))  # Remove duplicates\n",
        "\n",
        "    if tags:\n",
        "        updated_tags = [tag for tag in tags if tag not in original_tags]\n",
        "        original_tags.extend(updated_tags)\n",
        "        return original_tags\n",
        "    else:\n",
        "        tags = fallback_to_llm(feedback_text, original_tags)\n",
        "        updated_tags = [tag for tag in tags if tag not in original_tags]\n",
        "        original_tags.extend(updated_tags)\n",
        "        return original_tags\n",
        "\n",
        "\n",
        "#################################### Streamlit App ###################################################\n",
        "\n",
        "st.set_page_config(page_title=\"ExplainTrip\", layout=\"wide\", initial_sidebar_state=\"collapsed\")\n",
        "st.title(\"ExplainTrip!\")\n",
        "\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "if 'image_tags' not in st.session_state:\n",
        "    st.session_state['image_tags'] = None\n",
        "\n",
        "for msg in st.session_state.messages:\n",
        "    with st.chat_message(msg[\"role\"]):\n",
        "        st.write(msg[\"content\"])\n",
        "\n",
        "def fit_image_contain(img, target_width, target_height, color=(255, 255, 255)):\n",
        "    # Resize while keeping aspect ratio\n",
        "    img_ratio = img.width / img.height\n",
        "    target_ratio = target_width / target_height\n",
        "\n",
        "    if img_ratio > target_ratio:\n",
        "        # Image is wider, fit width\n",
        "        new_width = target_width\n",
        "        new_height = int(target_width / img_ratio)\n",
        "    else:\n",
        "        # Image is taller, fit height\n",
        "        new_height = target_height\n",
        "        new_width = int(target_height * img_ratio)\n",
        "\n",
        "    img_resized = img.resize((new_width, new_height), Image.LANCZOS)\n",
        "\n",
        "    # Create new canvas\n",
        "    new_img = Image.new(\"RGB\", (target_width, target_height), color)\n",
        "    paste_x = (target_width - new_width) // 2\n",
        "    paste_y = (target_height - new_height) // 2\n",
        "\n",
        "    new_img.paste(img_resized, (paste_x, paste_y))\n",
        "    return new_img\n",
        "\n",
        "def clean_message(llm_output: str) -> str:\n",
        "    if llm_output.startswith('Example: \"') and llm_output.endswith('\"'):\n",
        "        result = llm_output[len('Example: \"'): -1]\n",
        "    else:\n",
        "        result = llm_output  # or handle differently if format unexpected\n",
        "\n",
        "prompt = st.chat_input(placeholder=\"Got a favorite place? Let‚Äôs find its match!...\",accept_file=True, file_type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "if prompt:\n",
        "    if prompt.text and prompt.files:\n",
        "        uploaded_file = prompt.files[0]\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": f\"Uploaded file: {uploaded_file.name}\"})\n",
        "        with st.chat_message(\"user\"):\n",
        "            img = Image.open(uploaded_file)\n",
        "            fitted_img = fit_image_contain(img, 400, 300)  # Fit in 400x300 with white background\n",
        "            st.image(fitted_img, caption={uploaded_file.name}, use_container_width=False)\n",
        "            st.write(prompt.text)\n",
        "\n",
        "        time.sleep(1)\n",
        "        bot_response = \"Thanks for the image! Let me find similar places...\"\n",
        "        st.session_state.messages.append({\"role\": \"bot\", \"content\": bot_response})\n",
        "        with st.chat_message(\"bot\"):\n",
        "            st.write(bot_response)\n",
        "\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            # Simulated BLIP tags from VisionAgent\n",
        "            caption = segment_and_caption_image(img)\n",
        "            blip_tags = extract_keywords_from_caption(caption)\n",
        "            st.session_state['image_tags'] = blip_tags\n",
        "            # Match and display\n",
        "            results = match_from_blip_tags(blip_tags, top_k=25)\n",
        "            top_result = results[0]\n",
        "            llm_prompt = build_prompt(blip_tags, top_result)\n",
        "            # Print the generated text\n",
        "            #output = generator(llm_prompt, max_new_tokens=150, temperature=0.7)\n",
        "            #response_text = clean_message(output[0][\"generated_text\"])\n",
        "            response_text = call_llm()\n",
        "\n",
        "            bot_response = response_text\n",
        "            st.session_state.messages.append({\"role\": \"bot\", \"content\": bot_response})\n",
        "            with st.chat_message(\"bot\"):\n",
        "                st.write(bot_response)\n",
        "                response = requests.get(top_result[\"image_url\"])\n",
        "                result_img = Image.open(BytesIO(response.content))\n",
        "                fitted_result_img = fit_image_contain(result_img, 400, 300)  # Fit in 400x300 with white background\n",
        "                st.image(fitted_result_img, caption=f\"üìç {top_result['location']}\", use_container_width=False)\n",
        "    elif prompt.text:\n",
        "        user_text = prompt.text\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": user_text})\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.write(user_text)\n",
        "\n",
        "        time.sleep(1)\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            original_tags = st.session_state['image_tags']\n",
        "            feedback_text = user_text\n",
        "            updated_tags = process_feedback(feedback_text, original_tags)\n",
        "            # Match and display\n",
        "            results = match_from_blip_tags(updated_tags, top_k=25)\n",
        "            top_result = results[0]\n",
        "            llm_prompt = build_feedback_prompt(updated_tags, top_result)\n",
        "            # Print the generated text\n",
        "            #output = generator(llm_prompt, max_new_tokens=150, temperature=0.7)\n",
        "            #response_text = output[0][\"generated_text\"]\n",
        "            response_text = call_llm()\n",
        "\n",
        "            bot_response = response_text\n",
        "            st.session_state.messages.append({\"role\": \"bot\", \"content\": bot_response})\n",
        "            with st.chat_message(\"bot\"):\n",
        "                st.write(bot_response)\n",
        "                response = requests.get(top_result[\"image_url\"])\n",
        "                result_img = Image.open(BytesIO(response.content))\n",
        "                fitted_result_img = fit_image_contain(result_img, 400, 300)  # Fit in 400x300 with white background\n",
        "                st.image(fitted_result_img, caption=f\"üìç {top_result['location']}\", use_container_width=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsftlKBCmmWe",
        "outputId": "5d21811f-7e80-4b4e-d5da-a54efd2db912"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.91.129.20\n"
          ]
        }
      ],
      "source": [
        "# Your public ip is the password to the localtunnel\n",
        "!curl ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZVh7cCVmmWf",
        "outputId": "89b533dd-dadd-4d22-80cb-d8b0f146b332"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0Kyour url is: https://eight-birds-tease.loca.lt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py &>./logs.txt & npx localtunnel --port 8501"
      ]
    }
  ]
}